# Backend considerations for serving GPT-readable artifacts

This note summarizes the minimal backend (running on GCP) required for a Custom GPT to reliably read repo content, including the bundle, without changing the GPT prompt.

## Why a backend is needed
- The Custom GPT relies on a backend to read the latest `GPT_KNOWLEDGE.md`, setup steps, and bundle artifacts.
- Browsing direct GitHub URLs can be brittle (rate limits, raw URL changes). A backend gives you a stable, controlled endpoint and clear error handling when fetches fail.

## Build/source of truth
- Keep `npm run build` in GitHub Actions to regenerate `GPT_KNOWLEDGE.md` and the Apps Script bundle. Commit/push the artifacts so the backend always serves deterministic, reviewed outputs from `main`.
- Because builds happen in CI, the backend can stay read-only and serve static files; it does not need to run builds itself.

## Artifact scope (proposed minimal surface)
- `GPT_KNOWLEDGE.md` (regenerated by CI)
- `dist/apps_scripts_bundle.gs` (regenerated by CI)
- Other committed Markdown files (`*.md`) for deeper context when needed

Keeping the surface small lowers bandwidth and security risk.

## Usage docs as single source of truth
- Serve the usage guides directly from the backend to avoid manual GPT Knowledge updates and drift.
- Recommended endpoints:
  - `/usage/overview`
  - `/usage/super-admin`
  - `/usage/sheet-editor`

## Backend role and shape
- **Role:** act as a thin, read-only file server that returns the latest committed artifacts. It can fetch from GitHub (main branch) or from a mirrored bucket (e.g., Cloud Storage) populated by CI.
- **Shape:** a small HTTP service (Cloud Run) that returns files by path and responds with clear errors when a file is missing or stale. A reference implementation now lives at `backend/server.js`; see [Minimal backend for GPT file access](./BACKEND_SERVER.md) for how to run or deploy it.

## Trigger model (refreshing backend-visible artifacts)
- Since CI already commits built artifacts, the backend can fetch directly from `main` on demand. If you prefer caching/mirroring, use GitHub webhooks to push updated artifacts to storage after each `main` push, or run a lightweight periodic sync. Webhooks give fresher data but require auth and validation; schedules are simpler but may lag.

## MCP vs simple HTTP
- **Git MCP server:** useful if you later want the GPT to run commands or browse a virtual filesystem. Higher complexity (session mgmt, tool definitions) but future-proofs automation.
- **Simple HTTP server:** minimal overhead for read-only access to files. Best fit if you only need deterministic artifacts and no command execution.

## Language choice (JS vs Python)
- **JS/Node:** aligns with existing build tooling; easy reuse of repo helpers; lightweight Cloud Run image with `node:18/20`.
- **Python:** strong stdlib/http frameworks; good if you prefer minimal dependencies (`fastapi`, `flask`).

Pick the runtime your team supports best; functionality is similar for a thin file server.

## Security, reliability, and ops
- Use read-only GitHub credentials (deploy key or GitHub App token) or serve from a public bucket with signed URLs if acceptable.
- Require `BACKEND_API_KEY` in production; you may allow anonymous access in local development only.
- Add request logging, basic rate limiting (e.g., `RATE_LIMIT_MAX`/`RATE_LIMIT_WINDOW_MS`), and health checks.
- Fail closed: if the GPT cannot fetch the knowledge file, treat it as an error to be fixed, not something to silently ignore.
- Cost should be low on Cloud Run/Cloud Storage for light usage (likely near-zero at small scale).

## Next steps (if proceeding)
1. Finalize artifact list and storage location (direct from GitHub vs Cloud Storage mirror).
2. Add a minimal Cloud Run service that serves those artifacts with clear error responses.
3. Keep CI in charge of builds; the backend remains read-only.
4. Document the runtime fetch URL for `GPT_KNOWLEDGE.md` so the GPT prompt can reference it.
